{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-6\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, weights, biases, activations):\n",
    "    assert len(weights) == len(biases) == len(activations)\n",
    "    result = X\n",
    "    outputs = []\n",
    "    for W, b, activation in zip(weights, biases, activations):\n",
    "        layer_output = np.dot(result, W) + b\n",
    "        result = activation(layer_output)\n",
    "        outputs.append(result)\n",
    "    return result, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, t):\n",
    "    return -np.mean(t * np.log(y) + (1 - t) * np.log(1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, pred, target, weights, biases, outputs):\n",
    "    Ew = pred - target\n",
    "    Ev = outputs[0] * np.dot(weights[1], Ew)\n",
    "    loss = cross_entropy_loss(pred, target)\n",
    "    dW = np.outer(outputs[0], Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "    return loss, (dV, dW, Ev, Ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights, biases, activations):\n",
    "    return np.argmax(forward_prop(x, weights, biases, activations)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def fit(X, y):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    hidden_dim = 3\n",
    "    output_dim = 10\n",
    "    input_to_hidden_W = np.random.random((input_dim, hidden_dim))\n",
    "    input_to_hidden_b = np.random.random(hidden_dim)\n",
    "    hidden_to_output_W = np.random.random((hidden_dim, output_dim))\n",
    "    hidden_to_output_b = np.random.random(output_dim)\n",
    "\n",
    "    weights = [input_to_hidden_W, hidden_to_output_W]\n",
    "    biases = [input_to_hidden_b, hidden_to_output_b]\n",
    "    activations = [lambda x: x, sigmoid]\n",
    "    layers = weights + biases\n",
    "    err = [9999]\n",
    "    epoch = 0\n",
    "    while EPS < np.mean(err):\n",
    "        err = []\n",
    "        upd = [0] * len(layers)\n",
    "        for i in range(X.shape[0]):\n",
    "            predicted, outputs = forward_prop(X[i], weights, biases, activations)\n",
    "            loss, grad = back_prop(X[i], predicted, y[i], weights, biases, outputs)\n",
    "\n",
    "            for j in range(len(layers)):\n",
    "                layers[j] -= upd[j]\n",
    "\n",
    "            for j in range(len(layers)):\n",
    "                upd[j] = ALPHA * grad[j]\n",
    "            err.append(loss)\n",
    "        print 'Epoch {}: loss {}'.format(epoch, np.mean(err))\n",
    "        epoch += 1\n",
    "    return weights, biases, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.653046012994\n",
      "Epoch 1: loss 0.555681862236\n",
      "Epoch 2: loss 0.525769166129\n",
      "Epoch 3: loss 0.510282693288\n",
      "Epoch 4: loss 0.499600659827\n",
      "Epoch 5: loss 0.490598183377\n",
      "Epoch 6: loss 0.483521700664\n",
      "Epoch 7: loss 0.478367900947\n",
      "Epoch 8: loss 0.47466638584\n",
      "Epoch 9: loss 0.471966937645\n",
      "Epoch 10: loss 0.470214559162\n",
      "Epoch 11: loss nan\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishalyminov/.virtualenvs/interviewbit/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/home/ishalyminov/.virtualenvs/interviewbit/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n",
      "/home/ishalyminov/.virtualenvs/interviewbit/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/home/ishalyminov/.virtualenvs/interviewbit/lib/python2.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X = np.random.binomial(1, 0.5, (1000, 10))\n",
    "y = X ^ 1\n",
    "params = fit(X, y)\n",
    "print predict(X[0], *params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
